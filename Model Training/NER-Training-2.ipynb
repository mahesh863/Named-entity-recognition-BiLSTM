{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement: Recognize the Named Entity","metadata":{}},{"cell_type":"code","source":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense\nimport json","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:30.191786Z","iopub.execute_input":"2021-11-13T13:44:30.192065Z","iopub.status.idle":"2021-11-13T13:44:30.197077Z","shell.execute_reply.started":"2021-11-13T13:44:30.192014Z","shell.execute_reply":"2021-11-13T13:44:30.196082Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Loading and pre-processing the data","metadata":{}},{"cell_type":"code","source":"# Getting all the File Path\ndata_dir = \"../input/gmb-v220/gmb-2.2.0/data\"\nfile_names = []\nfor root, dirs, files in os.walk(data_dir):\n    for filename in files:\n        if filename.endswith(\".tags\"):\n            file_names.append(os.path.join(root, filename))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:30.198944Z","iopub.execute_input":"2021-11-13T13:44:30.199330Z","iopub.status.idle":"2021-11-13T13:44:37.740268Z","shell.execute_reply.started":"2021-11-13T13:44:30.199291Z","shell.execute_reply":"2021-11-13T13:44:37.739487Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"file_names[:2]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:37.742208Z","iopub.execute_input":"2021-11-13T13:44:37.742619Z","iopub.status.idle":"2021-11-13T13:44:37.748599Z","shell.execute_reply.started":"2021-11-13T13:44:37.742580Z","shell.execute_reply":"2021-11-13T13:44:37.747860Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Formatting the Named Entity Tokens in IOB format\ndef iob_formatter(ners):\n    iob_tokens = []\n    for idx, token in enumerate(ners):\n        if token != 'O':\n            if idx == 0:\n                token = \"B-\" + token\n            elif ners[idx-1] == token:\n                token = \"I-\" + token\n            else:\n                token = \"B-\" + token\n        iob_tokens.append(token)\n    return iob_tokens","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:37.751009Z","iopub.execute_input":"2021-11-13T13:44:37.751630Z","iopub.status.idle":"2021-11-13T13:44:37.757752Z","shell.execute_reply.started":"2021-11-13T13:44:37.751592Z","shell.execute_reply":"2021-11-13T13:44:37.756962Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Cleaning the NER tokens\ndef strip_ner(tag):\n    return tag.split(\"-\")[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:37.760411Z","iopub.execute_input":"2021-11-13T13:44:37.760669Z","iopub.status.idle":"2021-11-13T13:44:37.766742Z","shell.execute_reply.started":"2021-11-13T13:44:37.760644Z","shell.execute_reply":"2021-11-13T13:44:37.766066Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"all_data =[]\nfor idx, file in enumerate(file_names):\n    with open(file, 'rb') as content:\n        data = content.read().decode('utf-8').strip() # Reading from the files\n        sentences = data.split(\"\\n\\n\") # Splitting the sentences\n        for sentence in sentences:\n            toks = sentence.split('\\n') \n            words, ner = [], []\n            for tok in toks:\n                t = tok.split(\"\\t\") # Splitting the sentence and seperating the \"Named Entity\" from \"Words\"\n                words.append(t[0])\n                ner.append(strip_ner(t[3]))\n            all_data.append([\" \".join(words),\n                         \" \".join(iob_formatter(ner))])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:37.767529Z","iopub.execute_input":"2021-11-13T13:44:37.767703Z","iopub.status.idle":"2021-11-13T13:44:45.199259Z","shell.execute_reply.started":"2021-11-13T13:44:37.767681Z","shell.execute_reply":"2021-11-13T13:44:45.198467Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Initializing Tokenizers\ntext_tokenizer = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\nsplit=' ', oov_token='<OOV>')\n\nner_tokenizer = Tokenizer(filters='\\t\\n', lower=False,\nsplit=' ', oov_token='<OOV>')","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:45.212084Z","iopub.execute_input":"2021-11-13T13:44:45.212356Z","iopub.status.idle":"2021-11-13T13:44:45.221854Z","shell.execute_reply.started":"2021-11-13T13:44:45.212289Z","shell.execute_reply":"2021-11-13T13:44:45.220946Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(all_data, columns=[\"words\", \"ner\"]) #Converting the list to a dataframe","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:45.223340Z","iopub.execute_input":"2021-11-13T13:44:45.223621Z","iopub.status.idle":"2021-11-13T13:44:45.244656Z","shell.execute_reply.started":"2021-11-13T13:44:45.223582Z","shell.execute_reply":"2021-11-13T13:44:45.243608Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:45.250196Z","iopub.execute_input":"2021-11-13T13:44:45.250491Z","iopub.status.idle":"2021-11-13T13:44:45.260746Z","shell.execute_reply.started":"2021-11-13T13:44:45.250460Z","shell.execute_reply":"2021-11-13T13:44:45.259905Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"text_tokenizer.fit_on_texts(df['words'])\nner_tokenizer.fit_on_texts(df['ner'])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:45.262397Z","iopub.execute_input":"2021-11-13T13:44:45.263224Z","iopub.status.idle":"2021-11-13T13:44:47.734563Z","shell.execute_reply.started":"2021-11-13T13:44:45.263186Z","shell.execute_reply":"2021-11-13T13:44:47.733747Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"text_tokenizer_json = text_tokenizer.to_json()\nner_tokenizer_json = ner_tokenizer.to_json()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:47.736159Z","iopub.execute_input":"2021-11-13T13:44:47.736415Z","iopub.status.idle":"2021-11-13T13:44:47.871285Z","shell.execute_reply.started":"2021-11-13T13:44:47.736380Z","shell.execute_reply":"2021-11-13T13:44:47.870474Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"with open('txt_tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(text_tokenizer_json, ensure_ascii=False))\n    \nwith open('ner_tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(ner_tokenizer_json, ensure_ascii=False))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:47.873262Z","iopub.execute_input":"2021-11-13T13:44:47.873805Z","iopub.status.idle":"2021-11-13T13:44:47.917305Z","shell.execute_reply.started":"2021-11-13T13:44:47.873762Z","shell.execute_reply":"2021-11-13T13:44:47.916542Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Getting the config information from the tokenizer\nner_config = ner_tokenizer.get_config()\ntext_config = text_tokenizer.get_config()","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:47.918937Z","iopub.execute_input":"2021-11-13T13:44:47.919471Z","iopub.status.idle":"2021-11-13T13:44:48.270124Z","shell.execute_reply.started":"2021-11-13T13:44:47.919434Z","shell.execute_reply":"2021-11-13T13:44:48.269195Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"text_vocab = eval(text_config['index_word'])\nner_vocab = eval(ner_config['index_word'])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:48.271797Z","iopub.execute_input":"2021-11-13T13:44:48.272141Z","iopub.status.idle":"2021-11-13T13:44:48.454174Z","shell.execute_reply.started":"2021-11-13T13:44:48.272104Z","shell.execute_reply":"2021-11-13T13:44:48.453378Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# Converting to sequences\nx =  text_tokenizer.texts_to_sequences(df['words'])\ny =  ner_tokenizer.texts_to_sequences(df['ner'])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:48.455923Z","iopub.execute_input":"2021-11-13T13:44:48.456255Z","iopub.status.idle":"2021-11-13T13:44:50.074368Z","shell.execute_reply.started":"2021-11-13T13:44:48.456213Z","shell.execute_reply":"2021-11-13T13:44:50.073602Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Padding all the sentences to a fix length in order to feed the data to model\nmax_len = 50\nx_pad = sequence.pad_sequences(x, padding='post',\nmaxlen=max_len)\ny_pad = sequence.pad_sequences(y, padding='post',\nmaxlen=max_len)\nprint(x_pad.shape, y_pad.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:50.075896Z","iopub.execute_input":"2021-11-13T13:44:50.076181Z","iopub.status.idle":"2021-11-13T13:44:51.258540Z","shell.execute_reply.started":"2021-11-13T13:44:50.076144Z","shell.execute_reply":"2021-11-13T13:44:51.257722Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Converting the labels to binary vectors\nnum_classes = len(ner_vocab) + 1\nY = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\nY.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.259654Z","iopub.execute_input":"2021-11-13T13:44:51.259897Z","iopub.status.idle":"2021-11-13T13:44:51.419860Z","shell.execute_reply.started":"2021-11-13T13:44:51.259862Z","shell.execute_reply":"2021-11-13T13:44:51.419071Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"x_pad[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.421003Z","iopub.execute_input":"2021-11-13T13:44:51.421271Z","iopub.status.idle":"2021-11-13T13:44:51.426538Z","shell.execute_reply.started":"2021-11-13T13:44:51.421234Z","shell.execute_reply":"2021-11-13T13:44:51.425841Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nvocab_size = len(text_vocab) + 1\nembedding_dim = 64\nrnn_units = 100\nBATCH_SIZE=90\nnum_classes = len(ner_vocab)+1\ndropout=0.2","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.427780Z","iopub.execute_input":"2021-11-13T13:44:51.428185Z","iopub.status.idle":"2021-11-13T13:44:51.436900Z","shell.execute_reply.started":"2021-11-13T13:44:51.428150Z","shell.execute_reply":"2021-11-13T13:44:51.435917Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"X = x_pad\ntotal_sentences = X.shape[0]\ntest_size = round(total_sentences / BATCH_SIZE * 0.2)\nX_train = X[BATCH_SIZE*test_size:]\nY_train = Y[BATCH_SIZE*test_size:]\nX_test = X[0:BATCH_SIZE*test_size]\nY_test = Y[0:BATCH_SIZE*test_size]\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.438248Z","iopub.execute_input":"2021-11-13T13:44:51.438717Z","iopub.status.idle":"2021-11-13T13:44:51.446648Z","shell.execute_reply.started":"2021-11-13T13:44:51.438679Z","shell.execute_reply":"2021-11-13T13:44:51.445918Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n    model = tf.keras.Sequential([\n        Embedding(vocab_size, embedding_dim, mask_zero=True,\n                  batch_input_shape=[batch_size,None]),\n        Bidirectional(LSTM(units=rnn_units,\n                           return_sequences=True,\n                           dropout=dropout)),\n        TimeDistributed(Dense(rnn_units, activation='relu')),\n        Dense(num_classes, activation=\"softmax\")\n    ])\n    \n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.448057Z","iopub.execute_input":"2021-11-13T13:44:51.448343Z","iopub.status.idle":"2021-11-13T13:44:51.456613Z","shell.execute_reply.started":"2021-11-13T13:44:51.448293Z","shell.execute_reply":"2021-11-13T13:44:51.455843Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"model = build_model_bilstm(\n    vocab_size = vocab_size,\n    embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE,\n    classes=num_classes)\nmodel.summary()\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n              metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:51.457641Z","iopub.execute_input":"2021-11-13T13:44:51.458068Z","iopub.status.idle":"2021-11-13T13:44:52.979636Z","shell.execute_reply.started":"2021-11-13T13:44:51.458007Z","shell.execute_reply":"2021-11-13T13:44:52.978857Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Stopping early if model is not much improving\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience= 3)","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:52.980990Z","iopub.execute_input":"2021-11-13T13:44:52.981262Z","iopub.status.idle":"2021-11-13T13:44:52.986989Z","shell.execute_reply.started":"2021-11-13T13:44:52.981226Z","shell.execute_reply":"2021-11-13T13:44:52.986089Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15, callbacks= [early_stopping_cb], validation_data= (X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:44:52.988182Z","iopub.execute_input":"2021-11-13T13:44:52.988682Z","iopub.status.idle":"2021-11-13T13:45:55.094830Z","shell.execute_reply.started":"2021-11-13T13:44:52.988627Z","shell.execute_reply":"2021-11-13T13:45:55.093977Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"model.save(\"NER.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:45:55.096726Z","iopub.execute_input":"2021-11-13T13:45:55.097004Z","iopub.status.idle":"2021-11-13T13:45:55.213866Z","shell.execute_reply.started":"2021-11-13T13:45:55.096966Z","shell.execute_reply":"2021-11-13T13:45:55.212899Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val_loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.savefig(\"Loss.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:46:59.423100Z","iopub.execute_input":"2021-11-13T13:46:59.423365Z","iopub.status.idle":"2021-11-13T13:46:59.702048Z","shell.execute_reply.started":"2021-11-13T13:46:59.423333Z","shell.execute_reply":"2021-11-13T13:46:59.701251Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.ylabel(\"Acc\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.savefig(\"Acc.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-13T13:47:05.032993Z","iopub.execute_input":"2021-11-13T13:47:05.033698Z","iopub.status.idle":"2021-11-13T13:47:05.320592Z","shell.execute_reply.started":"2021-11-13T13:47:05.033662Z","shell.execute_reply":"2021-11-13T13:47:05.319876Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}